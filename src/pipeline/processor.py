import os
from typing import Dict, List, Union, Any, Optional
import logging
import json
from pathlib import Path
from .llm_client import generate_api_calls, SUPPORTED_APIS
from .csv_parser import CSVParser, ParsedRow, MethodType
from .batch_executor import BatchExecutor, ExecutionResult
import re
import random
import string
from .extractor import extract_api_calls
from csv2api.src.utils.logger import setup_logger

logger = logging.getLogger(__name__)
logger = setup_logger()

class PipelineProcessor:
    """Main pipeline processor for handling CSV data using LLM for event extraction."""
    
    SUPPORTED_APIS = {
        "get_events": {
            "description": "Fetches contract events based on address, event name, and signature."
        },
        "tag_as_expense": {
            "description": "Tags a transaction as an expense."
        },
        "get_transaction": {
            "description": "Fetches transaction details."
        },
        "get_receipt": {
            "description": "Fetches transaction receipt."
        }
    }

    def __init__(self, max_workers: int = 4):
        logger.error("PIPELINE_PROCESSOR_INIT_V_LATEST_R3: PipelineProcessor class is being initialized.") # DIAGNOSTIC LOG
        self.batch_executor = BatchExecutor(max_workers=max_workers)
        # self.supported_apis is now a class variable, no need to set it in __init__ unless it's instance-specific
    
    def _process_single_row(self, parsed_row_from_csv_parser: ParsedRow) -> Optional[Dict[str, Any]]:
        """
        Processes a single ParsedRow object (which contains raw_data from CSVParser).
        The raw_data is sent to the LLM for cleaning, validation, and API call structure generation.
        """
        try:
            logger.info(f"_process_single_row: Processing ParsedRow number {parsed_row_from_csv_parser.row_number}")
            api_calls = generate_api_calls(parsed_row_from_csv_parser.raw_data, debug=True) # from llm_client
            if not api_calls or not isinstance(api_calls, list) or len(api_calls) == 0:
                logger.warning(f"_process_single_row: Failed to generate API calls via LLM for row {parsed_row_from_csv_parser.row_number}. Raw data: {parsed_row_from_csv_parser.raw_data}")
                return None
            processed_api_call_struct = api_calls[0]
            if not self.validate_api_call(processed_api_call_struct):
                logger.warning(f"_process_single_row: LLM-generated API call failed validation for row {parsed_row_from_csv_parser.row_number}. Call: {processed_api_call_struct}")
                return None
            return {
                "row_number": parsed_row_from_csv_parser.row_number,
                "api_calls": [processed_api_call_struct],
                "raw_event_data_from_csv": parsed_row_from_csv_parser.raw_data
            }
        except Exception as e:
            logger.error(f"_process_single_row: Error processing row {parsed_row_from_csv_parser.row_number}: {str(e)}", exc_info=True)
            return None
    
    def process_file(self, file_path: str) -> List[Dict[str, Any]]:
        """
        Parses a CSV file and processes each row to generate API call structures.
        """
        try:
            # Step 1: Clean and normalize with extractor
            cleaned_rows = extract_api_calls(file_path)
            if not cleaned_rows:
                logger.error(f"process_file: No valid rows after extraction for file: {file_path}")
                return []
            # Step 2: Parse cleaned data with CSVParser
            parser = CSVParser(file_path='')  # file_path not needed for dict input
            parsed_rows = parser.parse_from_dicts(cleaned_rows)
            if not parsed_rows or not isinstance(parsed_rows, list):
                logger.error(f"process_file: No ParsedRow objects created by CSVParser for file: {file_path}")
                return []
            logger.info(f"process_file: CSVParser created {len(parsed_rows)} ParsedRow objects. Now processing with _process_single_row.")
            results = []
            for pr in parsed_rows:
                result = self._process_single_row(pr)
                if result:
                    results.append(result)
            if not results:
                logger.error("process_file: No rows were successfully processed by _process_single_row.")
                return []
            logger.info(f"process_file: Successfully processed {len(results)} out of {len(parsed_rows)} ParsedRows.")
            return results
        except Exception as e:
            logger.error(f"process_file: Error during file processing for {file_path}: {str(e)}", exc_info=True)
            return []

    def validate_api_call(self, api_call_struct: Dict) -> bool:
        """Validates a single API call structure (typically generated by the LLM client)."""
        method = api_call_struct.get('method')
        
        # DIAGNOSTIC LOGGING:
        logger.info(f"DIAGNOSTIC: Inside validate_api_call. Method to check: '{method}'. PipelineProcessor.SUPPORTED_APIS is: {PipelineProcessor.SUPPORTED_APIS}")

        # Ensure method is a string before checks
        if not isinstance(method, str):
            logger.error(f"validate_api_call: API 'method' is not a string or is missing. Call: {api_call_struct}")
            return False

        if method not in self.SUPPORTED_APIS: # Check instance first (though they should be same)
            if method not in PipelineProcessor.SUPPORTED_APIS: # Then check class variable directly
                logger.error(f"validate_api_call: Unsupported API method: '{method}'. Call: {api_call_struct}. (Checked against class var: {PipelineProcessor.SUPPORTED_APIS})")
                return False
            
        params = api_call_struct.get('params')
        if not isinstance(params, dict):
            logger.error(f"validate_api_call: 'params' field is missing or not a dict for method '{method}'. Call: {api_call_struct}")
            return False
        
        # Specific checks for 'get_events'
        if method == 'get_events':
            required_event_params_keys = {
                'contract_address': "a non-empty string starting with '0x'.",
                'event_name': "a non-empty string.",
                'event_signature': "a non-empty string containing '(' and ')'."
            }
            errors_found = []
            for p_key, p_desc in required_event_params_keys.items():
                val = params.get(p_key)
                if not val: # Covers None or empty string
                    errors_found.append(f"parameter '{p_key}' is missing or empty (expected {p_desc}) Got: '{val}'")
                elif p_key == 'contract_address' and (not isinstance(val, str) or not val.startswith('0x')):
                    errors_found.append(f"parameter '{p_key}' format is invalid (expected {p_desc}) Got: '{val}'")
                elif p_key == 'event_signature' and (not isinstance(val, str) or '(' not in val or ')' not in val):
                     errors_found.append(f"parameter '{p_key}' format is invalid (expected {p_desc}) Got: '{val}'")
                elif p_key == 'event_name' and (not isinstance(val, str) or not val): # just needs to be non-empty string
                     pass # Already covered by the initial `if not val:`
            
            if errors_found:
                logger.error(f"validate_api_call: Validation failed for 'get_events' due to: {'; '.join(errors_found)}. Params received: {params}")
                return False
        
        logger.debug(f"validate_api_call: Successfully validated API call for method '{method}'. Params: {params}")
        return True

    def _infer_function_from_prompt(self, prompt: str) -> Optional[str]:
        prompt_lower = prompt.lower()
        if any(kw in prompt_lower for kw in ['receipt', 'get receipt', 'fetch receipt', 'mark as receipt', 'retrieve receipt']):
            return 'get_receipt'
        if any(kw in prompt_lower for kw in ['tag', 'categorize', 'expense', 'mark as expense']):
            return 'tag_as_expense'
        if any(kw in prompt_lower for kw in ['transaction', 'get transaction', 'fetch transaction']):
            return 'get_transaction'
        if any(kw in prompt_lower for kw in ['fill account', 'deposit', 'top up']):
            return 'fill_account_by'
        if 'list chain' in prompt_lower:
            return 'list_chains'
        return None

    def _extract_first_json_array(self, text):
        # Remove markdown code block markers if present
        text = re.sub(r'```json|```', '', text)
        # Find the first JSON array in the text
        match = re.search(r'\[.*?\]', text, re.DOTALL)
        if match:
            return json.loads(match.group(0))
        raise ValueError('No JSON array found in LLM output')

    def _call_llm(self, prompt: str) -> str:
        import subprocess
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w+', delete=False) as tmpfile:
            tmpfile.write(prompt)
            tmpfile.flush()
            command = [
                "ollama", "run", "custom-mistral:instruct"
            ]
            process = subprocess.Popen(
                command,
                stdin=open(tmpfile.name, 'r'),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            stdout, stderr = process.communicate(timeout=120)
        logger.info(f"LLM STDOUT: {stdout}")
        if stderr:
            logger.error(f"LLM STDERR: {stderr}")
        if process.returncode != 0:
            logger.error(f"LLM process error. Return code: {process.returncode}. Stderr: {stderr.strip()}")
        return stdout

    def _get_mapping_plan_from_llm(self, prompt: str, headers: list) -> dict:
        api_docs = '\n'.join([
            f"- {name}: {SUPPORTED_APIS[name]['params']}" for name in SUPPORTED_APIS
        ])
        llm_prompt = f"""
You are an API router. Here are the available API functions:
{api_docs}

User prompt: {prompt}
Available columns: {headers}

CRITICAL REQUIREMENTS:
1. ONLY generate API calls that DIRECTLY fulfill the user's prompt.
2. DO NOT add any extra API calls beyond what was explicitly requested.
3. If the user asks to "fill accounts", ONLY use the fill_account_by API.
4. If the user asks to "get transactions", ONLY use the get_transaction API.
5. If the user asks to "tag expenses", ONLY use the tag_as_expense API.
6. If the user asks to "get receipts", ONLY use the get_receipt API.
7. If the user asks to "list chains", ONLY use the list_chains API.
8. Each API call must have ALL required parameters mapped to actual columns.
9. NEVER combine multiple API calls unless explicitly requested.
10. When in doubt, use ONLY the most relevant API that matches the user's intent.

Output a mapping plan in JSON that ONLY includes the API calls needed for the user's prompt.
Example format:
{{
  "api_calls": [
    {{
      "method": "fill_account_by",
      "params": {{"account_id": "tx_hash", "amount": "expense_category"}}
    }}
  ]
}}

Output ONLY a valid JSON object as specified above. Do NOT include any comments, explanations, markdown, or empty strings for required parameters."""
        logger.info(f"Sending mapping plan prompt to LLM:\n{llm_prompt}")
        mapping_plan_str = self._call_llm(llm_prompt)
        # Remove markdown code block markers if present
        mapping_plan_str = re.sub(r'```json|```', '', mapping_plan_str)
        # Remove // ... comments
        mapping_plan_str = re.sub(r'//.*', '', mapping_plan_str)
        match = re.search(r'\{.*\}', mapping_plan_str, re.DOTALL)
        if match:
            return json.loads(match.group(0))
        raise ValueError('No JSON object found in LLM output for mapping plan.')

    def process_natural_language(self, prompt: str, list_of_raw_data_dicts: list) -> tuple[Optional[str], list]:
        logger.info(f"Processing natural language prompt: {prompt}")
        logger.info(f"Input data contains {len(list_of_raw_data_dicts)} rows")
        if not list_of_raw_data_dicts:
            logger.error("No data rows provided.")
            return None, []

        # Determine intended function from prompt
        intended_function = self._infer_function_from_prompt(prompt)
        if intended_function:
            logger.info(f"Inferred intended function from prompt: {intended_function}")
        else:
            logger.warning("Could not infer intended function from prompt, will not filter API calls")

        # Define robust column mappings for common synonyms
        COLUMN_MAPPINGS = {
            'tx_hash': ['tx_hash', 'transaction_hash', 'hash', 'txn_hash', 'txhash', 'transaction id', 'txid'],
            'chain': ['chain', 'blockchain', 'network', 'platform'],
            'expense_category': ['expense_category', 'category', 'purpose', 'type', 'description', 'expense type']
        }

        # Function to find the actual column name based on canonical name
        def find_column(canonical_name: str, available_columns: list) -> Optional[str]:
            synonyms = COLUMN_MAPPINGS.get(canonical_name, [canonical_name])
            for col in available_columns:
                if col.lower().strip() in [s.lower() for s in synonyms]:
                    return col
            return None

        # Normalize the input data with canonical column names
        headers = list(list_of_raw_data_dicts[0].keys())
        normalized_data = []
        
        for row in list_of_raw_data_dicts:
            normalized_row = {}
            # Map known columns to their canonical names
            for canonical, _ in COLUMN_MAPPINGS.items():
                actual_col = find_column(canonical, row.keys())
                if actual_col:
                    normalized_row[canonical] = row[actual_col]
            # Keep any unmapped columns as is
            for k, v in row.items():
                if k not in normalized_row.values():
                    normalized_row[k] = v
            normalized_data.append(normalized_row)

        list_of_raw_data_dicts = normalized_data
        headers = list(normalized_data[0].keys())
        print(f"Normalized columns for LLM mapping: {headers}")

        try:
            mapping_plan = self._get_mapping_plan_from_llm(prompt, headers)
            print(f"LLM mapping plan: {mapping_plan}")
            logger.info(f"LLM mapping plan: {mapping_plan}")
        except Exception as e:
            logger.error(f"Failed to get mapping plan from LLM: {e}")
            mapping_plan = None
        api_calls_per_row = []
        if mapping_plan and 'api_calls' in mapping_plan:
            for row_num, row in enumerate(list_of_raw_data_dicts, 1):
                row_api_calls = []
                for api_call_plan in mapping_plan['api_calls']:
                    method = api_call_plan['method']
                    
                    # Filter out API calls that don't match the intended function
                    if intended_function and method != intended_function:
                        logger.info(f"Row {row_num}: Filtering out {method} API call as it doesn't match intended function {intended_function}")
                        continue
                        
                    param_map = api_call_plan['params']
                    params = {}
                    for param, col in param_map.items():
                        params[param] = row.get(col, None)
                    # Fill placeholders if present (robust)
                    for k, v in params.items():
                        if isinstance(v, str):
                            v_stripped = v.strip()
                            if re.match(r'<.*>', v_stripped):
                                if 'chain' in k:
                                    params[k] = 'ETHEREUM'
                                    logger.warning(f"Row {row_num}: Placeholder for '{k}' replaced with 'ETHEREUM'. (was: {v_stripped})")
                                elif 'tx_hash' in k:
                                    params[k] = ''.join(random.choices(string.hexdigits, k=64))
                                    logger.warning(f"Row {row_num}: Placeholder for '{k}' replaced with random string. (was: {v_stripped})")
                            elif v_stripped == '' or v_stripped.upper() in ['POLYGON', 'ETHEREUM', '[CHAIN]', '[CHAI}']:
                                if k == 'chain':
                                    params[k] = 'ETHEREUM'
                                    logger.warning(f"Row {row_num}: Empty or invalid string for 'chain' replaced with 'ETHEREUM'.")
                                elif k == 'tx_hash':
                                    params[k] = ''.join(random.choices(string.hexdigits, k=64))
                                    logger.warning(f"Row {row_num}: Empty or invalid string for 'tx_hash' replaced with random string.")
                    # Only add if all required params are present and non-empty
                    required = SUPPORTED_APIS.get(method, {}).get('params', [])
                    missing = []
                    for p in required:
                        v = params.get(p)
                        if v is None:
                            missing.append(p)
                        elif isinstance(v, str) and v.strip() == '':
                            missing.append(p)
                    if not missing:
                        row_api_calls.append({'method': method, 'params': params})
                    else:
                        logger.warning(f"Row {row_num}: Missing required params for {method}: {params}. Missing: {missing}")
                if row_api_calls:
                    api_calls_per_row.append({'row': row_num, 'api_calls': row_api_calls})
        else:
            logger.warning("Mapping plan missing or ambiguous, falling back to row-by-row LLM inference (not implemented here)")
        api_calls_per_row = [r for r in api_calls_per_row if r.get('row', 1) != 0]
        all_methods = set()
        for row in api_calls_per_row:
            for call in row.get('api_calls', []):
                method = call.get('method')
                if method:
                    all_methods.add(method)
        if all_methods:
            logger.info(f"API calls generated for methods: {', '.join(sorted(all_methods))}")
            function_summary = ', '.join(sorted(all_methods))
        else:
            logger.warning("No valid API calls generated.")
            function_summary = None
        return function_summary, api_calls_per_row

    def process_api_call(api_func, params):
        logger.debug(f"Calling API function: {api_func.__name__} with params: {params}")
        result = api_func(**params)
        logger.debug(f"API call result: {result}")
        return result

# Helper function for external use
async def process_with_llm(extracted_data: Union[Dict, List[Dict]], prompt: str = "tag these transactions as expenses") -> List[Dict]:
    """
    Process extracted data using LLM to identify and structure API calls.
    
    Args:
        extracted_data: Dictionary or list of dictionaries containing extracted data
        prompt: Natural language prompt describing what to do with the data (default: tag as expenses)
        
    Returns:
        List of structured API calls
    """
    processor = PipelineProcessor()
    if isinstance(extracted_data, dict):
        extracted_data = [extracted_data]
    return processor.process_natural_language(prompt, extracted_data)[1]  # Return just the API calls

if __name__ == "__main__":
    import asyncio
    import tempfile
    import csv

    # --- Standard Logging Setup ---
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler() # Ensure logs go to console
        ]
    )
    logger.info("PipelineProcessor test script started.")

    # --- Test 1: process_file (simulates CSV file processing) ---
    logger.info("--- Starting Test 1: process_file ---")
    # Example CSV row data
    csv_row_data_for_file_test = {
        "contract_address": "0x1111111111111111111111111111111111111111",
        "event thingy": "Transfer(address", # Intentionally incomplete signature part
        "extra_info": "address,uint256)"   # Rest of signature
    }
    processor_for_file_test = PipelineProcessor()
    temp_csv_file_path = ""

    try:
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=csv_row_data_for_file_test.keys())
            writer.writeheader()
            writer.writerow(csv_row_data_for_file_test)
            writer.writerow({"contract_address": "invalid_address", "event thingy": "Test(uint256)", "extra_info": ""}) # An invalid row
            temp_csv_file_path = f.name
        
        logger.info(f"Test CSV file created at: {temp_csv_file_path}")
        file_processing_results = processor_for_file_test.process_file(temp_csv_file_path)
        
        if file_processing_results:
            logger.info("Successfully processed test CSV file via process_file:")
            print(json.dumps(file_processing_results, indent=2))
        else:
            logger.warning("process_file: No results or failed to process test CSV file.")
    except Exception as e:
        logger.error(f"Error during process_file test: {e}", exc_info=True)
    finally:
        if temp_csv_file_path and os.path.exists(temp_csv_file_path):
            os.unlink(temp_csv_file_path)
            logger.info(f"Test CSV file deleted: {temp_csv_file_path}")
    logger.info("--- Finished Test 1: process_file ---")

    # --- Test 2: process_data (simulates direct data processing via process_with_llm helper) ---
    logger.info("--- Starting Test 2: process_data (via process_with_llm) ---")
    # Example raw data for direct processing (simulating already parsed/extracted data)
    raw_event_data_for_direct_test = [
        {
            "contract_address": "0x2222222222222222222222222222222222222222",
            "event_name": "Approval", # More direct field name
            "event_params": "(address,address,uint256)" # Complete params
        },
        {
            "contract_address": "0x3333333333333333333333333333333333333333",
            "some_event_col": "Payment(uint256,string)", # Different column name for event
            "notes": "A test payment event"
        },
        {
             "address_field": "invalid-address-format", # Invalid data
             "event_field": "Failure()"
        }
    ]

    async def run_process_data_test():
        try:
            logger.info(f"Test data for process_data: {json.dumps(raw_event_data_for_direct_test, indent=2)}")
            # process_with_llm calls processor.process_data()
            test_prompt = "tag these transactions as expenses"
            logger.info(f"Testing with prompt: {test_prompt}")
            direct_processing_results = await process_with_llm(raw_event_data_for_direct_test, test_prompt)
            if direct_processing_results:
                logger.info("Successfully processed test data via process_with_llm:")
                print(json.dumps(direct_processing_results, indent=2))
            else:
                logger.warning("process_with_llm: No results or failed to process test data.")
        except Exception as e:
            logger.error(f"Error during process_data test: {e}", exc_info=True)
        logger.info("--- Finished Test 2: process_data ---")

    asyncio.run(run_process_data_test())
    logger.info("PipelineProcessor test script finished.")