import os
from typing import Dict, List, Union, Any, Optional
import logging
import json
from pathlib import Path
from .llm_client import generate_api_calls, SUPPORTED_APIS
from .csv_parser import CSVParser, ParsedRow, MethodType
from .batch_executor import BatchExecutor, ExecutionResult
import re
import random
import string

logger = logging.getLogger(__name__)

class PipelineProcessor:
    """Main pipeline processor for handling CSV data using LLM for event extraction."""
    
    SUPPORTED_APIS = {
        "get_events": {
            "description": "Fetches contract events based on address, event name, and signature."
        },
        "tag_as_expense": {
            "description": "Tags a transaction as an expense."
        },
        "get_transaction": {
            "description": "Fetches transaction details."
        },
        "get_receipt": {
            "description": "Fetches transaction receipt."
        }
    }

    def __init__(self, max_workers: int = 4):
        logger.error("PIPELINE_PROCESSOR_INIT_V_LATEST_R3: PipelineProcessor class is being initialized.") # DIAGNOSTIC LOG
        self.batch_executor = BatchExecutor(max_workers=max_workers)
        # self.supported_apis is now a class variable, no need to set it in __init__ unless it's instance-specific
    
    def _process_single_row(self, parsed_row_from_csv_parser: ParsedRow) -> Optional[Dict[str, Any]]:
        """
        Processes a single ParsedRow object (which contains raw_data from CSVParser).
        The raw_data is sent to the LLM for cleaning, validation, and API call structure generation.
        """
        try:
            # parsed_row_from_csv_parser.raw_data contains canonically keyed raw CSV strings
            logger.info(f"_process_single_row: Processing ParsedRow number {parsed_row_from_csv_parser.row_number}")
            api_calls = generate_api_calls(parsed_row_from_csv_parser.raw_data, debug=True) # from llm_client
            
            if not api_calls:
                logger.warning(f"_process_single_row: Failed to generate API calls via LLM for row {parsed_row_from_csv_parser.row_number}. Raw data: {parsed_row_from_csv_parser.raw_data}")
                return None
            
            # Expecting one API call structure for event data
            processed_api_call_struct = api_calls[0]

            # Validate the structure returned by the LLM client
            if not self.validate_api_call(processed_api_call_struct):
                logger.warning(f"_process_single_row: LLM-generated API call failed validation for row {parsed_row_from_csv_parser.row_number}. Call: {processed_api_call_struct}")
                return None

            return {
                "row_number": parsed_row_from_csv_parser.row_number,
                "api_calls": [processed_api_call_struct], # Keep it as a list
                "raw_event_data_from_csv": parsed_row_from_csv_parser.raw_data # For reference
            }
            
        except Exception as e:
            logger.error(f"_process_single_row: Error processing row {parsed_row_from_csv_parser.row_number}: {str(e)}", exc_info=True)
            return None
    
    def process_file(self, file_path: str) -> List[Dict[str, Any]]:
        """
        Parses a CSV file and processes each row to generate API call structures.
        """
        try:
            parser = CSVParser(file_path)
            parsed_rows = parser.parse() # List of ParsedRow objects
            
            if not parsed_rows:
                logger.error(f"process_file: No ParsedRow objects created by CSVParser for file: {file_path}")
                return []
            
            logger.info(f"process_file: CSVParser created {len(parsed_rows)} ParsedRow objects. Now processing with _process_single_row.")
            
            results = []
            for pr in parsed_rows:
                result = self._process_single_row(pr)
                if result:
                    results.append(result)
                # _process_single_row logs its own errors/warnings for failed rows
            
            if not results:
                logger.error("process_file: No rows were successfully processed by _process_single_row.")
                return []
            
            logger.info(f"process_file: Successfully processed {len(results)} out of {len(parsed_rows)} ParsedRows.")
            return results
            
        except Exception as e:
            logger.error(f"process_file: Error during file processing for {file_path}: {str(e)}", exc_info=True)
            return []

    def validate_api_call(self, api_call_struct: Dict) -> bool:
        """Validates a single API call structure (typically generated by the LLM client)."""
        method = api_call_struct.get('method')
        
        # DIAGNOSTIC LOGGING:
        logger.info(f"DIAGNOSTIC: Inside validate_api_call. Method to check: '{method}'. PipelineProcessor.SUPPORTED_APIS is: {PipelineProcessor.SUPPORTED_APIS}")

        # Ensure method is a string before checks
        if not isinstance(method, str):
            logger.error(f"validate_api_call: API 'method' is not a string or is missing. Call: {api_call_struct}")
            return False

        if method not in self.SUPPORTED_APIS: # Check instance first (though they should be same)
            if method not in PipelineProcessor.SUPPORTED_APIS: # Then check class variable directly
                logger.error(f"validate_api_call: Unsupported API method: '{method}'. Call: {api_call_struct}. (Checked against class var: {PipelineProcessor.SUPPORTED_APIS})")
                return False
            
        params = api_call_struct.get('params')
        if not isinstance(params, dict):
            logger.error(f"validate_api_call: 'params' field is missing or not a dict for method '{method}'. Call: {api_call_struct}")
            return False
        
        # Specific checks for 'get_events'
        if method == 'get_events':
            required_event_params_keys = {
                'contract_address': "a non-empty string starting with '0x'.",
                'event_name': "a non-empty string.",
                'event_signature': "a non-empty string containing '(' and ')'."
            }
            errors_found = []
            for p_key, p_desc in required_event_params_keys.items():
                val = params.get(p_key)
                if not val: # Covers None or empty string
                    errors_found.append(f"parameter '{p_key}' is missing or empty (expected {p_desc}) Got: '{val}'")
                elif p_key == 'contract_address' and (not isinstance(val, str) or not val.startswith('0x')):
                    errors_found.append(f"parameter '{p_key}' format is invalid (expected {p_desc}) Got: '{val}'")
                elif p_key == 'event_signature' and (not isinstance(val, str) or '(' not in val or ')' not in val):
                     errors_found.append(f"parameter '{p_key}' format is invalid (expected {p_desc}) Got: '{val}'")
                elif p_key == 'event_name' and (not isinstance(val, str) or not val): # just needs to be non-empty string
                     pass # Already covered by the initial `if not val:`
            
            if errors_found:
                logger.error(f"validate_api_call: Validation failed for 'get_events' due to: {'; '.join(errors_found)}. Params received: {params}")
                return False
        
        logger.debug(f"validate_api_call: Successfully validated API call for method '{method}'. Params: {params}")
        return True

    def _infer_function_from_prompt(self, prompt: str) -> Optional[str]:
        prompt_lower = prompt.lower()
        if any(kw in prompt_lower for kw in ['receipt', 'get receipt', 'fetch receipt', 'mark as receipt', 'retrieve receipt']):
            return 'get_receipt'
        if any(kw in prompt_lower for kw in ['tag', 'categorize', 'expense', 'mark as expense']):
            return 'tag_as_expense'
        if any(kw in prompt_lower for kw in ['transaction', 'get transaction', 'fetch transaction']):
            return 'get_transaction'
        if any(kw in prompt_lower for kw in ['fill account', 'deposit', 'top up']):
            return 'fill_account_by'
        if 'list chain' in prompt_lower:
            return 'list_chains'
        return None

    def _extract_first_json_array(self, text):
        # Remove markdown code block markers if present
        text = re.sub(r'```json|```', '', text)
        # Find the first JSON array in the text
        match = re.search(r'\[.*?\]', text, re.DOTALL)
        if match:
            return json.loads(match.group(0))
        raise ValueError('No JSON array found in LLM output')

    def _call_llm(self, prompt: str) -> str:
        import subprocess
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w+', delete=False) as tmpfile:
            tmpfile.write(prompt)
            tmpfile.flush()
            command = [
                "ollama", "run", "mistral:instruct"
            ]
            process = subprocess.Popen(
                command,
                stdin=open(tmpfile.name, 'r'),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            stdout, stderr = process.communicate(timeout=120)
        logger.info(f"LLM STDOUT: {stdout}")
        if stderr:
            logger.error(f"LLM STDERR: {stderr}")
        if process.returncode != 0:
            logger.error(f"LLM process error. Return code: {process.returncode}. Stderr: {stderr.strip()}")
        return stdout

    def _get_mapping_plan_from_llm(self, prompt: str, headers: list) -> dict:
        api_docs = '\n'.join([
            f"- {name}: {SUPPORTED_APIS[name]['params']}" for name in SUPPORTED_APIS
        ])
        llm_prompt = f"""
You are an API router. Here are the available API functions:\n{api_docs}\n\nUser prompt: {prompt}\n\nHere are the column headers: {headers}\n\nFor this dataset, which API(s) should be called, and how should each column be mapped to the required parameters?\nOutput a mapping plan in JSON.\nExample format:\n{{\n  \"api_calls\": [\n    {{\n      \"method\": \"get_receipt\",\n      \"params\": {{\"chain\": \"chain\", \"tx_hash\": \"account_id\"}}\n    }}\n  ]\n}}\nOutput ONLY a valid JSON object as specified above. Do NOT include any comments, explanations, markdown, or empty strings for required parameters.\n"""
        logger.info(f"Sending mapping plan prompt to LLM:\n{llm_prompt}")
        mapping_plan_str = self._call_llm(llm_prompt)
        # Remove markdown code block markers if present
        mapping_plan_str = re.sub(r'```json|```', '', mapping_plan_str)
        # Remove // ... comments
        mapping_plan_str = re.sub(r'//.*', '', mapping_plan_str)
        match = re.search(r'\{.*\}', mapping_plan_str, re.DOTALL)
        if match:
            return json.loads(match.group(0))
        raise ValueError('No JSON object found in LLM output for mapping plan.')

    def process_natural_language(self, prompt: str, list_of_raw_data_dicts: list) -> tuple[Optional[str], list]:
        logger.info(f"Processing natural language prompt: {prompt}")
        logger.info(f"Input data contains {len(list_of_raw_data_dicts)} rows")
        if not list_of_raw_data_dicts:
            logger.error("No data rows provided.")
            return None, []
        headers = list(list_of_raw_data_dicts[0].keys())
        try:
            mapping_plan = self._get_mapping_plan_from_llm(prompt, headers)
            logger.info(f"LLM mapping plan: {mapping_plan}")
        except Exception as e:
            logger.error(f"Failed to get mapping plan from LLM: {e}")
            mapping_plan = None
        api_calls_per_row = []
        if mapping_plan and 'api_calls' in mapping_plan:
            for row_num, row in enumerate(list_of_raw_data_dicts, 1):
                row_api_calls = []
                for api_call_plan in mapping_plan['api_calls']:
                    method = api_call_plan['method']
                    param_map = api_call_plan['params']
                    params = {}
                    for param, col in param_map.items():
                        params[param] = row.get(col, None)
                    # Fill placeholders if present (robust)
                    for k, v in params.items():
                        if isinstance(v, str):
                            v_stripped = v.strip()
                            # Replace any placeholder of the form <...>
                            if re.match(r'<.*>', v_stripped):
                                if 'chain' in k:
                                    params[k] = 'ETHEREUM'
                                    logger.warning(f"Row {row_num}: Placeholder for '{k}' replaced with 'ETHEREUM'. (was: {v_stripped})")
                                elif 'tx_hash' in k:
                                    params[k] = ''.join(random.choices(string.hexdigits, k=64))
                                    logger.warning(f"Row {row_num}: Placeholder for '{k}' replaced with random string. (was: {v_stripped})")
                            elif v_stripped == '':
                                # Fill empty string for required params
                                if k == 'chain':
                                    params[k] = 'ETHEREUM'
                                    logger.warning(f"Row {row_num}: Empty string for 'chain' replaced with 'ETHEREUM'.")
                                elif k == 'tx_hash':
                                    params[k] = ''.join(random.choices(string.hexdigits, k=64))
                                    logger.warning(f"Row {row_num}: Empty string for 'tx_hash' replaced with random string.")
                    # Only add if all required params are present and non-empty
                    required = SUPPORTED_APIS.get(method, {}).get('params', [])
                    if all(params.get(p) for p in required):
                        row_api_calls.append({'method': method, 'params': params})
                    else:
                        logger.warning(f"Row {row_num}: Missing required params for {method}: {params}")
                if row_api_calls:
                    api_calls_per_row.append({'row': row_num, 'api_calls': row_api_calls})
        else:
            logger.warning("Mapping plan missing or ambiguous, falling back to row-by-row LLM inference (not implemented here)")
        # Remove any rows with 'row': 0 (LLM output bug)
        api_calls_per_row = [r for r in api_calls_per_row if r.get('row', 1) != 0]
        # After collecting all api_calls_per_row, summarize API methods
        all_methods = set()
        for row in api_calls_per_row:
            for call in row.get('api_calls', []):
                method = call.get('method')
                if method:
                    all_methods.add(method)
        if all_methods:
            logger.info(f"API calls generated for methods: {', '.join(sorted(all_methods))}")
            function_summary = ', '.join(sorted(all_methods))
        else:
            logger.warning("No valid API calls generated.")
            function_summary = None
        return function_summary, api_calls_per_row

# Helper function for external use
async def process_with_llm(extracted_data: Union[Dict, List[Dict]]) -> List[Dict]:
    """
    Process extracted data using LLM to identify and structure API calls.
    
    Args:
        extracted_data: Dictionary or list of dictionaries containing extracted data
        
    Returns:
        List of structured API calls
    """
    processor = PipelineProcessor()
    if isinstance(extracted_data, dict):
        extracted_data = [extracted_data]
    return processor.process_natural_language("", extracted_data)[1]  # Return just the API calls

if __name__ == "__main__":
    import asyncio
    import tempfile
    import csv

    # --- Standard Logging Setup ---
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler() # Ensure logs go to console
        ]
    )
    logger.info("PipelineProcessor test script started.")

    # --- Test 1: process_file (simulates CSV file processing) ---
    logger.info("--- Starting Test 1: process_file ---")
    # Example CSV row data
    csv_row_data_for_file_test = {
        "contract_address": "0x1111111111111111111111111111111111111111",
        "event thingy": "Transfer(address", # Intentionally incomplete signature part
        "extra_info": "address,uint256)"   # Rest of signature
    }
    processor_for_file_test = PipelineProcessor()
    temp_csv_file_path = ""

    try:
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=csv_row_data_for_file_test.keys())
            writer.writeheader()
            writer.writerow(csv_row_data_for_file_test)
            writer.writerow({"contract_address": "invalid_address", "event thingy": "Test(uint256)", "extra_info": ""}) # An invalid row
            temp_csv_file_path = f.name
        
        logger.info(f"Test CSV file created at: {temp_csv_file_path}")
        file_processing_results = processor_for_file_test.process_file(temp_csv_file_path)
        
        if file_processing_results:
            logger.info("Successfully processed test CSV file via process_file:")
            print(json.dumps(file_processing_results, indent=2))
        else:
            logger.warning("process_file: No results or failed to process test CSV file.")
    except Exception as e:
        logger.error(f"Error during process_file test: {e}", exc_info=True)
    finally:
        if temp_csv_file_path and os.path.exists(temp_csv_file_path):
            os.unlink(temp_csv_file_path)
            logger.info(f"Test CSV file deleted: {temp_csv_file_path}")
    logger.info("--- Finished Test 1: process_file ---")

    # --- Test 2: process_data (simulates direct data processing via process_with_llm helper) ---
    logger.info("--- Starting Test 2: process_data (via process_with_llm) ---")
    # Example raw data for direct processing (simulating already parsed/extracted data)
    raw_event_data_for_direct_test = [
        {
            "contract_address": "0x2222222222222222222222222222222222222222",
            "event_name": "Approval", # More direct field name
            "event_params": "(address,address,uint256)" # Complete params
        },
        {
            "contract_address": "0x3333333333333333333333333333333333333333",
            "some_event_col": "Payment(uint256,string)", # Different column name for event
            "notes": "A test payment event"
        },
        {
             "address_field": "invalid-address-format", # Invalid data
             "event_field": "Failure()"
        }
    ]

    async def run_process_data_test():
        try:
            logger.info(f"Test data for process_data: {json.dumps(raw_event_data_for_direct_test, indent=2)}")
            # process_with_llm calls processor.process_data()
            direct_processing_results = await process_with_llm(raw_event_data_for_direct_test)
            
            if direct_processing_results:
                logger.info("Successfully processed data via process_data/process_with_llm:")
                print(json.dumps(direct_processing_results, indent=2))
            else:
                logger.warning("process_data/process_with_llm: No results or failed to process data.")
        except Exception as e:
            logger.error(f"Error during process_data test: {e}", exc_info=True)

    asyncio.run(run_process_data_test())
    logger.info("--- Finished Test 2: process_data (via process_with_llm) ---")
    logger.info("PipelineProcessor test script finished.")